---
title: "TESTING Ais_scrape APPROACHES"
author: "Sean Goral"
date: "10/20/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(tidyverse)
library(rvest)
library(RCurl)
library(XML)
library(readr)
#library(magicfor)               

```

```{r, BASIC COLUMN SETUP FROM read.R}

 ais_col_types <- cols(
    X1 = col_character(),
    X2 = col_character(),
    X3 = col_character(),
    X4 = col_double(),
    X5 = col_double(),
    X6 = col_character(),
    X7 = col_character(),
    X8 = col_character(),
    X9 = col_character(),
    X10 = col_double(),
    X11 = col_double(),
    X12 = col_double(),
    X13 = col_double(),
    X14 = col_double(),
    X15 = col_double(),
    X16 = col_double(),
    X17 = col_character(),
    X18 = col_double(),
    X19 = col_double(),
    X20 = col_double(),
    X21 = col_double(),
    X22 = col_double(),
    X23 = col_double(),
    X24 = col_double(),
    X25 = col_double()
  )

```

Scrape a day of AIS DATA into DF

```{r, Extract AIS data: 1 day}

get_ais_text = function(ais_text){
  
    hours.list = c(0:23)
    hours.list_1 = sprintf('%02d', hours.list)
    
    year.list = c(2018:2022)
    year.list1 = sprintf('%d', year.list)
    
    ais_text = lapply(paste0('https://ais.sbarc.org/logs_delimited/2019/190101/AIS_SBARC_190101-', hours.list_1,'.txt'),
                    function(url){
                      url %>% 
                        read_delim(";", col_names = sprintf("X%d", 1:25), col_types = ais_col_types)
                      
                    })
  DF = do.call(rbind.data.frame, ais_text)
  return(DF)
}

get_ais_text()

```
Gets all website AIS numbers
```{r}

get_ais_list = function(ais_list){
  
year.list = c(2018:2022)
ais_list = lapply(paste0("https://ais.sbarc.org/logs_delimited/", year.list, "/"),
                function(url){
                  url %>% 
                    getURL() %>% 
                      htmlParse(asText = TRUE) %>%
                        xpathSApply("//a", xmlValue) %>% 
                          gsub("[^0-9]", "", .) %>% 
                            cat(paste())
                })
}

get_ais_list()

#needs to write into list... or something

```
Gets all website AIS numbers and puts into dataframe
```{r}

get_ais_list1 = function(ais_list){
  
  year.list = c(2018:2022)
  
    webpage = (paste0("https://ais.sbarc.org/logs_delimited/", year.list, "/"))
    urls = getURL(webpage) 
    text = htmlParse(urls, asText = TRUE) 
    values = xpathSApply(text, "//a", xmlValue)
    ais_list = gsub("[^0-9]", "", values) %>% 
      as.numeric()
    ais_list = ais_list[!is.na(ais_list)]
    
    ais_df = data.frame(ais_list)
    
    return(ais_df)
}

get_ais_list()
```


```{r, 2019 MANUAL NUMBER ENTRY}

trip.list = c(190101:191016)
trip.list_1 = sprintf("%d", trip.list)

try(
  lapply(paste0('https://ais.sbarc.org/logs_delimited/2019/', trip.list_1, '/AIS_SBARC_', trip.list_1,'-00.txt'),
                function(url1){
                  url1 %>% 
                    read_delim(";", col_names = sprintf("X%d", 1:25), col_types = ais_col_types)
                  
                })
)

#url issue
```

```{r, for loop, manual number entry}

for (i in 180101:181016) {
  for (j in 0:23) {
  #ERROR HANDLING
  possibleError <- tryCatch(
      thing(),
      error=function(e) e
  )

  if(inherits(possibleError, "error")) next

  #REAL WORK
  
  i1 = sprintf('%d', i)
  j1 =sprintf('%02d', j)
  url1 = paste0("https://ais.sbarc.org/logs_delimited/2018/", i1,"/AIS_SBARC_", i1,"-", j1,".txt")
   data0 = read_html(url1, "pre") %>%
     html_text() %>% 
     read_delim(";", col_names = sprintf("X%d", 1:25), col_types = ais_col_types)
}
  }#end for

```

